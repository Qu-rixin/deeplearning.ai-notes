# Setting up your ML application

## 1. Train /dev /test sets

​		最佳决策取决于所拥用的数据量，计算机配置中输入特征的数量。

​		循环该过程的效率是决定项目进展速度的一个关键因素，而创建高质量的训练数据集、验证集和测试集也有助于提高循环效率。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Train_dev_test_sets1.PNG)

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Train_dev_test_sets2.PNG)

​		对训练集执行训练算法，然后通过验证集选择最好的模型，经过充分验证选定了最终模型，最后在测试集上进行评估。

​		现在的数据量可能是百万级别的，那么验证集和测试集占数据总量的比例会趋向于变得更小，因为验证集的目的就是验证不同的算法那种更有效，因此验证集要足够大才能评估（比如100万条数据选1万条数据充当验证集）。

​		测试集的主要目的是正确评估分类器的性能（比如100万条数据选1千条数据充当测试集）。

​		总结：在机器学习中，通常将样本分成训练集、验证集和测试集三部分。数据集规模相对较小时，适用于传统的划分比例；数据集规模较大时，验证集和测试集可以占到数据总量的20%和10%以下。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Train_dev_test_sets3.PNG)

​		现代深度学习的另一个趋势是，越来越多的人在训练集和测试集分布不匹配的情况下进行训练。

​		要确保验证集和测试集的数据来自同一分布：因为要用验证集来评估不同的模型，尽可能地优化性能，如果验证集和测试集来自同一分布，但由于深度学习需要大量的训练数据，为了获取更大规模的训练数据集，我们可以采用当前流行的各种创意策略，例如网页抓取，代价就是训练集数据与验证集和测试集数据，有可能不是来自同一分布。

​		最后一点，就算没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏评估，如果不需要无偏评估也可以不设置测试集，所以如果只有验证集，没有测试集，我们要做的是，在训练集上训练，尝试不同的模型架构，在验证集上评估这些模型，然后迭代选出适用的模型，因为验证集中已经涵盖测试集数据。

## 2. Bias/Variance

左：欠拟合，中：适度拟合，右：过拟合

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Bias_Variance1.PNG)

​		在这样只有x1和x2两个特征的二维数据集中，我们可以绘制数据，将偏差和方差可视化，在多维空间数据中，绘制数据和可视化分割边界无法实现，但可以通过几个指标来研究偏差和方差。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Bias_Variance2.PNG)

​		理解偏差和方差的两个关键数据是：训练集误差和验证集误差。

​		从图中第一种情况可以看出，可能过拟合了训练集，某种程度上，验证集并没有充分利用交叉验证集的作用，这种情况，我们称之为“高偏差”，通过查看训练集误差和验证集误差，我们便可以诊断算法是否具有**高偏差**，也就是说，衡量训练集和验证集误差得出不同结论。

​		从图中第二种情况可以看出，算法没有在训练集中得到很好的训练，如果训练数据的**拟合度不高，就是数据欠拟合**，就可以说这种算法**偏差比较高**，相反，它对于验证集产生的结果却是合理的。

​		从图中第三种情况可以看出，它训练集上结果不理想，方差也很高，这是**方差、偏差都很糟糕**的情况。

​		从图中第四种情况可以看出，**偏差、方差都很低**。

​		这些分析都是基于假设预测的，假设人眼辨别的错误率接近0%，一般来说，最优误差也被称为基本误差。当图片很模糊，即使是人眼或者没有系统可以准确无误地识别图片，这种情况下，基本误差会更高，那么分析过程就要做些改变了。

​		**重点：查看训练集误差，可以判断数据拟合情况，可以判断是否有偏差问题，然后通过查看验证集错误率有多高，可以判断方差是否过高，以上分析的前提都是假设基本误差很小，训练集和验证集数据来自相同分布。**

## 3. Basic “Recipe” for machine learning

​		训练完模型后，首先要知道算法的偏差高不高，如果偏差较高试着评估训练集或训练数据的性能，如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新网络，比如含有更多的隐层或隐层单元的网络，或者花费更多时间训练算法，或者尝试更先进的优化算法，也许有用，也许没用，不过采用规模更大的网络通常都会有所帮助，延长训练时间不一定有用，但也没有坏处，训练算法时，要不断尝试这些方法，直到解决掉偏差问题（拟合数据），这是最低标准，至少拟合训练集。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Basic_Recipe_for_machine_learning.PNG)

​		为了评估方差，我们要查看验证集性能，我们能从一个性能理想的训练集推断出验证集的性能也理想。如果方差高，最好的解决方法就是采用更多数据，如果无法获得更多数据，我们可以通过正则化来减少过拟合，如果能找到更合适的神经网络框架，同时减少偏差和方差，重要的是不断尝试，直到找到一个低偏差、低方差的框架。

​		我们通常用训练验证集来诊断算法是否存在偏差或方差问题。

​		在当前深度学习和大数据时代，只要持续训练一个更大的网络，只要准备了更多数据可以在减少一方的同时不影响另一方。

# Regularizing your neural network

## 1. Regularization

​		如果神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多的数据，但你可能无法时时准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或者减少网络误差。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Regularization1.PNG)

​		w通常是一个高维参数矢量，已经可以表达高偏差问题，w可能含有很多参数，我们不可能拟合所有参数，而b只是单个数字，所以w几乎涵盖所有参数，而不是b，如果加入了参数b，其实也没什么太大影响，因为b只是众多参数中的一个，所以我通常忽略不计，如果你想加上这个参数，完全没问题。

​		如果用的是L1正则化，w最终会是稀疏的，也就是说w中有很多0，所以人们在训练网络时，越来越倾向于使用L2正则化。

​		通常用验证集来配置这个参数（λ），把参数正常值设置为较小值，这样可以避免过拟合，另外λ是一个需要调整的超参数。注：λ是python的关键字，在编写代码时，我们删掉a写成lambd。

Frobenius范数（F-范数）：

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Regularization2.PNG)

## 2. Why regularization reduce overfitting?

​		为什么压缩L2范数可以减少过拟合，直观上理解就是如果正则化λ设置的足够大，权重矩阵w被设置为接近于0的值，直观理解就是把多隐层单元的权重设为0，于是消除了这些隐层单元的许多影响，这个被大大简化的神经网络会变成很小的网络，小到一个如同逻辑回归单元（实际上是这些隐层单元影响变小了），可是深度却很大，它会使这个网络从过拟合的状态更接近左图的高偏差状态，但是λ会存在一个中间值，于是会更接近中间状态。

​		直观理解就是λ增加到足够大，w会接近于0，实际上是不会发生这种情况的。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Why_regularization_reduce_overfitting1.PNG)

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Why_regularization_reduce_overfitting2.PNG)

​		总结：如果正则化参数变得很大，参数w很小，z也会相对变小，此时忽略b的影响，实际上z的取值范围很小，这个激活函数会相对呈线性，整个神经网络会计算离线性函数近的值，不会发生过拟合。

​		建议：在增加正则化项时，应用之前定义的代价函数J，我们增加了一项，目的是预防权重过大，如果使用梯度下降时，代价函数对于梯度下降的每一个调幅都单调递减；如果实施的是正则化函数，请牢记，J已经有一个全新的定义 。

## 3. Dropout regularization

​		假设左图的神经网络存在过拟合，dropout（随机失活）会遍历网络的每一层，并设置消除神经网络中节点的概率，假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删掉从该节点进出的连线，最后得到一个节点更少、规模更小的网络，然后用反向传播的方法进行训练。针对每个样本训练规模极小的网络。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Dropout_regularization1.PNG)

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Dropout_regularization2.PNG)

invert dropout（反向随机失活（最常用的dropout））：

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Dropout_regularization3.PNG)

​		d^3表示一个三层的dropout向量，keep-prob表示保留某个隐层单元的概率。

​		第一句的作用就是生成随机矩阵。d^3是一个矩阵，每个样本和每个隐层单元，其在d^3中的对应值为1的概率都是0.8，其对应值为0的概率是0.2。随机数字小于0.8，它等于1的概率是0.8，等于0的概率是0.2，接下来要做的是从第三层中获取激活函数。

​		第二句的作用就是过滤d^3所有等于0的元素。

​		dropout方法的功能是，不论keep-prob的值是多少，反向随即失活方法通过除以keep-prob确保a3的期望值不变。

​		不同的训练样本清除的隐层单元也不同，事实上，如果你通过相同的训练集多次传递数据，每次训练数据的梯度都不同，则随机对不同隐层单元归零，有时却并非如此。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Dropout_regularization4.PNG)

​		在测试阶段进行预测时，我们不期望输出结果是随机的，如果预测阶段应用dropout函数，预测会受到干扰。理论上，你只需要多次预测处理过程，每一次不同的隐层单元会被随机归零，预测处理遍历它们，但计算效率低，得出的结果也几乎相同，与这个不同程序产生的结果极为相似。

​		反向随机失活函数在除以keep-prob时可以记住上一步的操作，目的是确保即使在测试阶段不执行dropout来调整数值范围，激活函数的预期结果也不会发生变化，所以在测试阶段没必要添加额外的尺度函数，这与训练阶段不同。

## 4. Understanding dropout

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Understanding_dropout.PNG)

​		通过dropout，紫色单元能依赖任何特征，因为特征都有可能被随机清除，不愿意给任何一个输入加上太多权重，因为它可能被删除，因此该单元通过这种方式积极地传播开来，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和我们讲过的L2正则化类似，实施dropout的结果是它会压缩权重，并完成一些预防过拟合的外层正则化，事实证明dropout被正式作为一种正则化的替代形式，L2对不同权重的衰减是不同的，它取决于倍增激活函数的大小。

​		总结：dropout的功能类似于L2正则化，与L2正则化不同的是，被应用的方式不同，dropout也会有所不同，甚至更适用于不同的输入范围，实施dropout的另一个细节是：keep-prob代表每一层上保留单元的概率，所以不同层的keep-prob也可以变化，w[2]是最大的权重矩阵，因为w[2]拥有最大的参数集，即7x7，为了预防矩阵的过拟合，对于这一层，它的keep-prob值应该相对较低，对于其他层，过拟合程度可能没那么严重，它们的keep-prob值可能高一些，如果在某一层，我们不担心其过拟合的问题，那么keep-prob可以为1。

​		注意keep-prob的值是1，意味着保留所有单元，并且不在这一层使用dropout，对于有可能出现过拟合且含有诸多参数的层，我们可以把keep-prob设置成比较小的值，以便应用更强大的dropout。

​		从技术上将，我们也可以对输入层应用dropout，我们有机会删除一个或多个输入特征，虽然在现实中，我们通常不这么做，keep-prob的值为1，是非常常用的输入值。

​		实施dropout在计算机视觉领域有很多成功的第一次，计算机视觉的输入量非常大，输入太多的像素，以至于没有足够多的数据，所以dropout在计算机视觉领域中应用得比较频繁，但要牢记一点，dropout是一种正则化方法，它有助于过拟合，因此除非算法过拟合，不然不会使用dropout，所以它在其他领域应用的比较少，主要存在于计算机视觉领域，因为我们没有足够多的数据，所以一直存在过拟合。

​		dropout的一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，事实上是很难进行复查的，定义明确的代价函数J，每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者在某种程度上很难计算，所以我们失去了调试工具来绘制图中左下角的图，我们通常会关闭dropout函数，将keep-prob的值设成1，运行代码，确保J函数单调递减，然后再打开dropout函数。

## 5. Other regularization methods

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Other_regularization_methods1.PNG)

​		通过水平翻转图片，训练集可以增大一倍，因为训练集有冗余，这虽然不如我们额外搜集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。除了上面的方法，你也可以随意裁剪图片。

​		像这样人工合成数据，我们要通过算法验证。

​		所以**数据扩增**可作为正则化方法使用，实际功能也与正则化相似。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Other_regularization_methods2.PNG)

​		还有另外一种常用的方法叫做early stopping，运行梯度下降时，我们可以绘制训练误差或只绘制代价函数J的优化过程，在训练集上用0-1记录分类误差次数呈单调下降趋势。通过early stopping，我们不单可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差或验证集上的代价函数、逻辑损失和对数损失等。你会发现，验证集误差通常会先成下降趋势，然后在某个节点处开始上升，early stopping的作用是，你会说，神经网络已经在这个迭代过程中表现得很好了，我们在此停止训练吧，得到验证集误差，它是怎么发挥作用的，当你还未在神经网络上运行太多迭代过程的时候，参数w接近0，因为随机初始化w值时，它的值可能都是较小的随机值，所以你在长期训练神经网络之前，w依然很小，在迭代和训练过程中，w的值会变得越来越大，所以early stopping要做得就是在中间点停止迭代的过程，我们得到一个w值中等大小的F范数，与L2正则化相似，选择参数w范数较小的神经网络。术语early stopping代表提前停止神经网络训练。

​		early stopping的缺点：机器学习的几个步骤，其中一步是选择一个算法来优化代价函数J，我们有很多工具来解决这个问题，如梯度下降，但优化代价函数J之后，我们也不想发生过拟合，有一些工具可以解决该问题，比如正则化、扩增数据等。在机器学习中，超参数激增，选出可行的算法也变得越来越复杂，如果我们用一组工具优化代价函数J，机器学习就会变得很简单，在重点优化代价函数J时，只需要留意w和b，J(w,b)的值越小越好，你只需想办法减少这个值，其他的不用关注，然后预防过拟合还有其他任务，换句话说就是减少方差，这一步我们用另一个工具来实现，这个原理有时被称为正交化，思路就是在一个时间做一个任务。early stopping的主要缺点就是你不能独立的处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，因为现在你不再尝试降低代价函数J，所以代价函数J的值可能不够小，同时你有希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是另一种方法同时解决两个问题，这样做的结果是，我们要考虑的东西变得更复杂。如果不用early stopping，另一种方法就是L2正则化，训练神经网络的时间就可能很长，这导致超参数搜索空间更容易分解，但缺点就是你必须尝试很多的正则化参数λ的值，这也导致搜索大量λ值的计算代价太高，early stopping的优点是只运行一次梯度下降，你可以找出w的较小值、中间值和较大值，而无需尝试L2正则化超参数λ的很多值。

# Setting up your optimization problem

## 1. Normalizing inputs

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Normalizing_inputs1.PNG)

​		训练神经网络，其中一个加速的方法就是归一化输入，假设我们有一个训练集，它有两个输入特征，所以输入特征x是二维的，归一化输入需要两个步骤，第一步是零均值化，第二步是归一化方差（注意特征x1的方差比特征x2的方差要大得多）。提示一下，如果你用它来调整训练数据，那么用相同的μ和σ^2来归一化测试集，尤其是你不希望训练集和测试集的归一化有所不同，不论μ值是什么，也不论σ^2的值是什么，这两个公式中会用到它们，所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估μ和σ^2，因为我们希望不论是训练数据还是测试数据都是通过相同的μ和σ^2定义的相同数据转换，其中μ和σ^2是由训练集数据计算得来的。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Normalizing_inputs2.PNG)

​		为什么我们想归一化输入特征？

​		如果特征值在不同的范围，结果是参数w1和w2值的范围或比率将会非常不同，然而如果你归一化特征，代价函数平均起来看更对称，如果你在左图这样的代价函数上运行梯度下降法，你必须使用一个非常小的学习率，因为如果是在这个位置（左下角的图），梯度下降法可能需要多次迭代过程，直到最后找到最小值，但如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，你可以在梯度下降法中使用较大步长，而不需要像在左图中那样反复执行，当然w是一个高维向量，因此用二维绘制w并不能正确地传达直观理解，但总的直观理解是代价函数会更圆一些，而且更容易优化，前提是特征都是在相似范围内，而不是从1到1000，0到1的范围，而是在-1到1的范围内或相似偏差，这使得代价函数J优化起来更简单更快速。实际上，如果特征在相似范围，所以会表现得很好，当它们在非常不同的取值范围，这对优化算法非常不利。

## 2. Vanishing/exploding gradients

​		训练神经网络，尤其是深度神经网络所面临的一个问题是梯度消失或梯度爆炸，也就是说，当你训练神经网络时导数或坡度有时会变得非常大或非常小，甚至以指数的方式变小，这加大了训练的难度。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Vanishing_exploding_gradients.PNG)

​		权重w只比1略大一点或者说只比单位矩阵大一点，深度神经网络的激活函数将爆炸式增长，如果w比1略小一点，在深度神经网络中，激活函数将以指数级递减。尤其是梯度与L相差指数级，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习。

## 3. Weight initialization for deep networks

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Weight_initialization_for_deep_networks.PNG)

​		注：左下角的紫色方框中的公式是方差的平方根。

​		为了预防z值过大或过小，你可以看到n越大，你希望wi越小，因为z是wixi的和，如果你把很多此类项相加，希望每项值更小，最合理的方法就是设置wi=1/n，n表示神经元的输入特征数量，实际上，你要做的就是设置某层权重矩阵w如图，如果你使用ReLU激活函数（g()）（Var为方差），将方差设置为2/n更好。

​		你常常会发现，初始化时，尤其是使用ReLU激活函数时，g[l] (z)=ReLU(z)，它取决于你对随机变量的熟悉程度，这是高斯随机变量然后乘以它的平方根，也就是引用这个方差2/n，这里用的是n^[l-1]，因为本例中，逻辑回归的特征是不变的，但一般情况下，l层上的每一个神经元都有n^[l-1]个输入，如果激活函数的输入特征被零均值和标准方差，方差是1，z也会调整到相似范围，这就没解决问题，但它确实降低了梯度降低和爆炸的问题，因为它给权重w设置了合理值，你也知道，它不能比1大很多，也不能比1小很多，所以梯度没有爆炸或消失过快。

​		刚刚提到的是ReLU函数，对于其他几个变体函数，如tanh激活函数，有篇论文提出常量1比常量2的效率更高，对于tanh函数来说，它是1/n^([l-1])的平方根，这里平方根的作用与左下角的紫色方框中的公式作用相同，它适用于tanh激活函数，被称为Xavier初始化。Yoshua Bengio和他的同事还提出另一种方法，他们使用的公式是2/(n^[l-1]+n^[l])的平方根，其它理论已对此证明。但如果你想用ReLU激活函数也就是最常用的激活函数，通常用左下角的紫色方框中的公式。

​		但是所有这些公式只是给你一个起点，它们给出初始化权重矩阵的方差的默认值，如果你想添加方差，方差参数则是另一个你需要调整的超参数，可以给左下角的紫色方框中的公式添加一个乘数参数，调优作为超参数激增一份子的乘子参数，有时调优该超参数效果一般，这并不是我想调优的首要超参数，但我已经发现调优过程中产生的问题，虽然调优该参数能起到一定作用，但考虑到相比调优其他超参数的重要性，我通常把它的优先级放得比较低 。

​		我们在训练深度神经网络时，这也是一个加快训练速度的技巧。

## 4. Numerical approximation of gradients

​		在实施backprop时，有个测试叫梯度检验，它的作用是确保backprop正确实施，因为有时候你虽然写下这些方程式，却不能100%确定，执行backprop的所有细节都是正确的，为了逐渐实现梯度检验，首先理解计算梯度做数值逼近。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Numerical_approximation_of_gradients1.PNG)

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Numerical_approximation_of_gradients2.PNG)

​		较大三角形的高宽比值更接近于θ的导数，把右上角的三角形下移，所以我们得到的不是一个单边公差而是一个双边公差。在前一张ppt中，我们只考虑了单边公差，即从θ到θ+ε之间的误差，所以使用双边公差的方法更逼近导数。现在我们更加确信，g(θ)可能是一个f导数的正确实现，在梯度检验和反向传播中使用该方法时，最终，它与运行两次单边公差的速度一样。实际上，我认为这种方法还是非常值得使用的，因为它的结果更准确。

​		所以在执行梯度检验时，我们使用双边公差，而不是用单边误差，因为它不够准确。

​		我们可以用这个方法来检验反向传播是否得以正确实施，如果不正确可能有bug

## 5. Gradient Checking

​		梯度检验节省时间，可以发现backprop实施过程中的bug。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Gradient_Checking1.PNG)

​		为执行梯度检验，首先要做的就是把所有参数转换成一个巨大的向量数据，你要做的是把矩阵w转换成一个向量，把所有的w矩阵转换成向量之后，做连接运算，得到一个巨型向量θ，代价函数是所有w和b的函数，现在你得到了一个θ的代价函数J，接着，你得到与w和b顺序相同的数据。接着，可以把所有导数转换成一个大向量dθ，它与θ具有相同的维度，现在的问题是dθ、代价函数的梯度有什么关系。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Gradient_Checking2.PNG)

​		首先要清楚J是超参数θ的一个函数，你也可以将J函数展开为J(θ1, θ2, θ3, ...)，不论超参数θ向量的维度是多少。

​		为了实施梯度检验，你要做的就是循环执行，从而对每个i也就是对每个θ组成的元素计算dθapprox[i]的值，我使用双边误差，也就是θ的J函数。要做的就是验证这些向量是否彼此接近，具体来说，如何定义两个向量是否真的接近彼此，一般做下列运算：

​		计算这两个向量的距离，然后用向量长度做归一化，分母只是预防这些向量太小或太大，分母使得整个方程式变成比率，我们实际执行这个方程，ε值可能成为10的-7次方，使用这个取值范围内的ε，如果你发现计算方程式得到的值为10的-7次方或更小，这就很好，这意味着导数逼近很有可能是正确的，它的值非常小，如果它的值在10的-5次方范围内，就要小心了，也许这个值没问题，但我会再次检查这个向量的所有项，确保没有一项误差过大，如果有一项误差非常大，这里可能有bug，如果左边这个方程式结果是10的-3次方，我就会担心是否存在bug，计算结果应该比10的-3次方小很多，这时应该仔细检查所有θ项，看是否有一个具体的i值使得dθapprox和dθ大不相同，并用它来追踪一些求导计算是否正确，经过一些调试，最终，结果会是这种非常小的值，那么你的实施可能是正确的，在实施神经网络时，经常需要执行foreprop和backprop，然后可能发现这个梯度检验有一个相对较大的值，然后会存在bug并不断调试，调试一段时间后，得到一个很小的梯度检验值，最后可以认为神经网络是正确的。

## 6. Gradient checking implementation notes

​				如何在神经网络实施梯度的实用技巧和注意事项：

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/02-Improving_Deep_Neural_Networks/week1/images/Gradient_checking_implementation_notes.PNG)

+ 首先，不要在训练中使用梯度检验，它只用于调试，意思是计算所有i值的dθapprox[i]，是一个非常漫长的计算过程，为了实施梯度下降，你必须使用backprop来计算dθ，并使用backprop来计算导数，只有调试的时候，你才会计算它，来确认数值是否接近dθ，完成后，你会关闭梯度检验，梯度检验的每一个迭代过程都不执行它，因为它太慢了。
+ 第二点如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出bug，也就是说，如果dθapprox[i]与dθ的值相差很大，我们要做的就是查找不同的i值，看看是哪导致的，dθapprox[i]与θ的值相差这么多，举个例子，如果你发现，相对于某些层或某层的db[l]，θ或dθ的值相差很大，但是dw[l]的各项非常接近，注意θ的各项与b和w的各项都是一一对应的，这时，你可能会发现，在计算参数b的导数db的过程中存在bug，反过来也一样，如果你发现它们的值相差很大，dθapprox的值与dθ相差很大，你会发现所有这些项都来自dw或某层的dw，可以帮你定位bug的位置，虽然未必能够帮你准确定位bug的位置，但它可以帮你估测需要在哪些地方追踪bug。
+ 第三点，在实施梯度检验时，如果使用正则化，请注意正则项s，如果代价函数J等于如图，dθ等于与θ相关的J函数的梯度，包括这个正则项。
+ 第四点，梯度检验不能与dropout同时使用，因为每次迭代过程中，dropout会随机消除隐层单元的不同子集，难以计算dropout在梯度下降上的代价函数J，因此dropout可以作为优化代价函数J的一种方法，但是代价函数J被定义为对所有指数极大的节点子集求和，而在任何迭代过程中，这些节点都有可能被消除，所以很难计算代价函数J，你只是对成本函数做抽样，用dropout每次随机消除不同的子集，所以很难用梯度检验来双重检验dropout的计算，所以一般不同时使用梯度检验和dropout，如果你想这样做，可以把dropout中的keep.prob设置为1，然后打开dropout并寄望于dropout的实施是正确的，你还可以做点别的，比如修改节点丢失的模式，实际上，一般不这么做，所以建议关闭dropout使用梯度检验进行双重检查，在没有dropout的情况下，你的算法至少是正确的，然后打开dropout。
+ 最后一点，也是比较微妙的一点，现实中几乎不会存在这种情况，当w和b接近0时，梯度下降的实施是正确的，在随机初始化过程中，但是在运行梯度下降时，w和b变得更大，可能只有在w和b接近0的时候，backprop的实施才是正确的，但是当w和b变大时，它会变得越来越不准确，你需要做一件事，就是在随机初始化过程中，运行梯度检验，然后在训练网络，w和b会有一段时间远离0，如果随机初始化值比较小，反复训练网络之后，再重新运行梯度检验。
