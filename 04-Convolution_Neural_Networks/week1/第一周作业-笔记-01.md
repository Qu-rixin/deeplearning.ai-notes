# Convolutional Neural Networks: Step by Step

​		你将用numpy实现卷积层和池化层，包括前向传递和后向传递。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\Notation1.PNG)

## 1. Outline of the Assignment

​		你将实现一个卷积神经网络的构造块！你将实现的每个功能都将有详细的说明，指导您完成所需的步骤：

+ 卷积函数：
  + 零填充
  + 卷积窗口
  + 前向卷积
  + 向后卷积（可选）

+ 池化函数：
  + 向前池化
  + 创建掩码
  + 分配值
  + 后向池化（可选）

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\model.png)

​		注意：对于每一个前向函数，有相对应的后向函数，因此，在前向模块的每一步，你将存储参数在缓存中，这些参数被用来计算梯度和后向传递。

## 2. Convolutional Neural Networks

​		尽管编程框架使卷积易于使用，但它们仍然是深度学习中最难理解的概念之一。卷积层将输入转换成不同大小的输出，如下所示。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\conv_nn.png)

​		在这一部分中，你将构建卷积层的每个步骤。首先实现两个辅助函数：一个用于零填充，另一个用于计算卷积函数本身。

### 1. Zero-Padding

​		零填充在图像边框周围添加零：

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\PAD.png)

​		填充的主要好处如下：

+ 它允许你使用卷积层，而不必缩小立方的高度和宽度。这对于构建更深层的网络很重要，因为否则，当你进入更深的层时，高度/宽度将缩小。一个重要的特殊情况是“相同”卷积，其中高度/宽度在一层之后被精确地保留。

+ 它有助于我们在图像边界保留更多的信息。如果没有填充，下一层的值很少会受到图像边缘像素的影响。

```python
def zero_pad(X, pad):
    """
    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, 
    as illustrated in Figure 1.
    
    Argument:
    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images
    pad -- integer, amount of padding around each image on vertical and horizontal dimensions
    
    Returns:
    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)
    """
    
    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)
    
    return X_pad

np.random.seed(1)
x = np.random.randn(4, 3, 3, 2)
x_pad = zero_pad(x, 2)
print ("x.shape =", x.shape)
print ("x_pad.shape =", x_pad.shape)
print ("x[1,1] =", x[1,1])
print ("x_pad[1,1] =", x_pad[1,1])

fig, axarr = plt.subplots(1, 2)
axarr[0].set_title('x')
axarr[0].imshow(x[0,:,:,0])
axarr[1].set_title('x_pad')
axarr[1].imshow(x_pad[0,:,:,0])
plt.show()
```

结果：

x.shape = (4, 3, 3, 2)
x_pad.shape = (4, 7, 7, 2)
x[1,1] = [[ 0.90085595 -0.68372786]
 [-0.12289023 -0.93576943]
 [-0.26788808  0.53035547]]
x_pad[1,1] = [[0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]
 [0. 0.]]

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\Zero-Padding1.png)

### 2. Single step of convolution

​		在这一部分中，实现一步卷积，在卷积中，你将过滤器应用于输入的单个位置。这将用于构建一个卷积单元，它：

+ 获取输入立方

+ 在输入的每个位置应用过滤器

+ 输出另一个立方（通常大小不同）

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\Convolution_schematic.gif)

​		在计算机视觉应用中，左边矩阵中的每一个值对应一个像素值，我们将其值元素与原始矩阵相乘，然后求和并加上偏差，从而将3x3过滤器与图像卷积。在本练习的第一步中，你将实现一个卷积步骤，对应于仅对其中一个位置应用滤波器以获得单个实值输出。

​		在本笔记的后面，你将把这个函数应用到输入的多个位置，以实现完全卷积操作。

```python
def conv_single_step(a_slice_prev, W, b):
    """
    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation 
    of the previous layer.
    
    Arguments:
    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)
    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)
    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)
    
    Returns:
    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data
    """

    # Element-wise product between a_slice and W. Do not add the bias yet.
    s = np.multiply(a_slice_prev, W)
    # Sum over all entries of the volume s.
    Z = np.sum(s)
    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.
    Z = Z + float(b)

    return Z


np.random.seed(1)
a_slice_prev = np.random.randn(4, 4, 3)
W = np.random.randn(4, 4, 3)
b = np.random.randn(1, 1, 1)

Z = conv_single_step(a_slice_prev, W, b)
print("Z =", Z)
```

结果：

Z = -6.99908945068

### 3. Convolutional Neural Networks - Forward pass

​		在前向传递中，你将使用许多过滤器并将它们卷积到输入上。每个“卷积”给你一个二维矩阵输出。然后，将这些输出叠加以获得三维立方。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\vert_horiz_kiank.png)

```python
def conv_forward(A_prev, W, b, hparameters):
    """
    Implements the forward propagation for a convolution function
    
    Arguments:
    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)
    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)
    b -- Biases, numpy array of shape (1, 1, 1, n_C)
    hparameters -- python dictionary containing "stride" and "pad"
        
    Returns:
    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)
    cache -- cache of values needed for the conv_backward() function
    """
    
    # Retrieve dimensions from A_prev's shape (≈1 line)  
    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
    
    # Retrieve dimensions from W's shape (≈1 line)
    (f, f, n_C_prev, n_C) = W.shape
    
    # Retrieve information from "hparameters" (≈2 lines)
    stride = hparameters['stride']
    pad = hparameters['pad']
    
    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)
    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1
    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1
    
    # Initialize the output volume Z with zeros. (≈1 line)
    Z = np.zeros((m, n_H, n_W, n_C))
    
    # Create A_prev_pad by padding A_prev
    A_prev_pad = zero_pad(A_prev, pad)
    
    for i in range(m):                               # loop over the batch of training examples
        a_prev_pad = A_prev_pad[i]                    # Select ith training example's padded activation
        for h in range(n_H):                           # loop over vertical axis of the output volume
            for w in range(n_W):                       # loop over horizontal axis of the output volume
                for c in range(n_C):                   # loop over channels (= #filters) of the output volume
                    
                    # Find the corners of the current "slice" (≈4 lines)
                    vert_start = h * stride
                    vert_end = vert_start + f
                    horiz_start = w * stride
                    horiz_end = horiz_start + f
                    
                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)
                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]
                    
                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)
                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])
                                        
    
    # Making sure your output shape is correct
    assert(Z.shape == (m, n_H, n_W, n_C))
    
    # Save information in "cache" for the backprop
    cache = (A_prev, W, b, hparameters)
    
    return Z, cache


np.random.seed(1)
A_prev = np.random.randn(10,4,4,3)
W = np.random.randn(2,2,3,8)
b = np.random.randn(1,1,1,8)
hparameters = {"pad" : 2,
               "stride": 2}

Z, cache_conv = conv_forward(A_prev, W, b, hparameters)
print("Z's mean =", np.mean(Z))
print("Z[3,2,1] =", Z[3,2,1])
print("cache_conv[0][1][2][3] =", cache_conv[0][1][2][3])
```

结果：

Z's mean = 0.048995203528855794
Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437
  5.18531798  8.75898442]
cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]

## 3. Pooling layer

​		池化（pool）层减少了输入的高度和宽度。它有助于减少计算量，也有助于使特征检测器对其在输入中的位置更加不变性。两种类型的池层是：

+ 最大池化：在输入上滑动（f，f）窗口，并在输出中存储窗口的最大值。

+ 平均池化：在输入上滑动（f，f）窗口，并在输出中存储窗口的平均值。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\max_pool1.png)

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week1\images-zy\a_pool.png)

​		这些池化层没有用于反向传播训练的参数。但是，它们有超参数，如窗口大小f。这指定要计算最大值或平均值的fxf窗口的高度和宽度。

### 1. Forward Pooling

```python
def pool_forward(A_prev, hparameters, mode = "max"):
    """
    Implements the forward pass of the pooling layer
    
    Arguments:
    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)
    hparameters -- python dictionary containing "f" and "stride"
    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")
    
    Returns:
    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)
    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters 
    """
    
    # Retrieve dimensions from the input shape
    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
    
    # Retrieve hyperparameters from "hparameters"
    f = hparameters["f"]
    stride = hparameters["stride"]
    
    # Define the dimensions of the output
    n_H = int(1 + (n_H_prev - f) / stride)
    n_W = int(1 + (n_W_prev - f) / stride)
    n_C = n_C_prev
    
    # Initialize output matrix A
    A = np.zeros((m, n_H, n_W, n_C))              
    
    ### START CODE HERE ###
    for i in range(m):                         # loop over the training examples
        for h in range(n_H):                     # loop on the vertical axis of the output volume
            for w in range(n_W):                 # loop on the horizontal axis of the output volume
                for c in range (n_C):            # loop over the channels of the output volume
                    
                    # Find the corners of the current "slice" (≈4 lines)
                    vert_start = h * stride
                    vert_end = vert_start + f
                    horiz_start = w * stride
                    horiz_end = horiz_start + f
                    
                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)
                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]
                    
                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.
                    if mode == "max":
                        A[i, h, w, c] = np.max(a_prev_slice)
                    elif mode == "average":
                        A[i, h, w, c] = np.mean(a_prev_slice)
    
    ### END CODE HERE ###
    
    # Store the input and hparameters in "cache" for pool_backward()
    cache = (A_prev, hparameters)
    
    # Making sure your output shape is correct
    assert(A.shape == (m, n_H, n_W, n_C))
    
    return A, cache



np.random.seed(1)
A_prev = np.random.randn(2, 4, 4, 3)
hparameters = {"stride" : 2, "f": 3}

A, cache = pool_forward(A_prev, hparameters)
print("mode = max")
print("A =", A)
print()
A, cache = pool_forward(A_prev, hparameters, mode = "average")
print("mode = average")
print("A =", A)
```

结果：

mode = max
A = [[[[ 1.74481176  0.86540763  1.13376944]]]


 [[[ 1.13162939  1.51981682  2.18557541]]]]

mode = average
A = [[[[ 0.02105773 -0.20328806 -0.40389855]]]


 [[[-0.22154621  0.51716526  0.48155844]]]]

#### 