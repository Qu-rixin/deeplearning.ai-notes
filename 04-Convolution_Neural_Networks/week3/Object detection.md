# Object detection

## 1. Object localization

​		为了建立对象检测，你需要先学习对象定位。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Object_localization1.PNG)

​		如果你想要定位图中的汽车，为了实现这个，你 可以改变你的神经网络，通过得到更多的输出单位，使得这个神经网络可以输出一个边框，所以，具体地说，你的神经网络要输出额外4个数字。在视频中，图片左上角坐标为（0，0），右下角坐标为（1，1），为了指定图中红色长方形边框，我们需要指定边框的中点，所以你的训练集不仅仅包含对象的类别标签，而且也包含4个额外数字。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Object_localization2.PNG)

​		目标标签y的定义如下，是一个向量，其中pc表示这里包含一个对象的概率，c1、c2、c3告诉我们这个是哪个类。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Object_localization3.PNG)

## 2. Landmark detection

​		在更普遍的例子中，你可以让神经网络只输出重要的点和图像的（x，y）坐标，比如你要建立一个脸部识别应用，你的算法告诉你某人眼角在哪里，那个点有个坐标，所以你可以建立一个神经网络，它的最后一层只输出两个数字lx、ly，现在，如果你想要神经网络告诉你眼角的所有4个角，那么我们可以修改神经网络，让它输出如图四个点。为了讨论方便，你可以在脸上定义64个坐标，并产生一个包含所有坐标的标签训练集合，然后你可以让神经网络告诉你，所有脸部重要的位置或重要的坐标在哪里。所以这是一个从脸部识别表情的基本模块。

​		当然为了有一个这样的网络，你需要一个有标签的训练集，这组训练集包含一组图像和标签y由人工枯燥地遍历并注释所有这些坐标。

​		如果你对人体姿势检测感兴趣，你也可以定义一些关键的位置建立一个神经网络去标识人的姿势的关键位置以及通过这个神经网络输出所有图中标识的点，你也可以让神经网络输出人的姿势。当然为了实现这个，你也需要指出这些关键特征就像l1x和l1y ...。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Landmark detection.PNG)

## 3. Object detection

​		假设你想搭建一个汽车检测算法，你可以先创建一个有标记训练数据集，x和对应的y，训练集包含紧密裁剪的汽车样例，然后你就可以训练一个卷积神经网络，当你训练好了这个神经网络，你就可以把它用在滑动窗口检测中。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Object_detection1.PNG)

​		如果你有一张这样的测试图像，你要做的是先选择一个如图所示的窗口尺寸，然后将一个小的矩形窗口输入到这个卷积神经网络，也就是说，只取小的红色框内的图像，把它输入到卷积神经网络中，然后让卷积神经网络做一个预测，假设对于如图红色框的区域，它会说那个红色小框里没有车，在滑动窗口检测算法，然后你要做的就是输入传递第二个图像把这个移动了一点的红色小框范围内的图像输入卷积神经网络，一直继续，直到你滑动窗口遍历了图像的所有位置，我们这里用了一个比较大的滑动步长（stride）。但基本思路是遍历所有这个红色窗口尺寸的图像区域，将这些小的裁剪出来的图像传递到卷积神经网络中，然后将一定步长下每个位置分类为0或1，现在，遍历完一次，你再重复一次，不过这次用更大的窗口，也就是说现在你取一个较大的区域，根据卷积神经网络的要求调整这个区域的大小，然后把它输入到卷积神经网络中，把这个窗口在一定步长下继续滑动遍历整个图像，然后你可以用更大的窗口再执行第三次。

​		我们的期望是，如果这么做只要图像中某处有一辆车，就会有某个窗口，比如说你输入这个窗口区域里的图像，我们希望卷积神经网络会输出1，这样你就检测到了这里有一辆车。

​		这个算法之所以被称为滑动窗口检测，是因为你把这些窗口在一定步长下滑过整个图像，然后判定每个方框内区域是否含有车。

​		不过滑动窗口检测有一个很大的缺陷，就是它的计算成本，因为你裁剪出了不同的正方形的图像，并让每个图像都单独通过卷积神经网络的运算，如果你使用了一个不密集的步长，一个很大的步长那么通过卷积神经网络的窗口数量会减少，但是比较粗粒度会影响算法表现，而如果你用了比较细粒度，也就是很小的步长，那么这些小区域的数量会巨大，将它们全部通过卷积神经网络意味着很高的计算成本，滑动窗口检测慢的不可行，而且除非你用很细粒度，也就是很小的步长，你最终很难在图像中精确地定位到目标。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Object_detection2.PNG)

​		幸运的是，计算成本问题有一个很好的解决方案，更确切地说，滑动窗口目标检测器，可以用卷积的形式来更高效的实现。

## 4. Convolutional implementation of sliding Windows

​		为了搭建一个用卷积实现的滑动窗口，让我们来看看如何将神经网络中的全连接层转化为卷积层。

​		如图，实现这一层全连接层的一个方式是用5x5过滤器，如果你拿一个5x5x16的图像和一个5x5过滤器做卷积操作，一个5x5过滤器实现时是用5x5x16的过滤器，因为按照惯例，这个过滤器要作用于全部16个通道，所以这个通道一一对应，这样输出就是1x1，如果你有400个这样的5x5x16的过滤器，那么输出的维度就是1x1x400，所以与其把这400个看作一组节点，还可以将它看做一个1x1x400的体，也就是说，400个值中每个都是这5x5x16个前一层的激活函数，输入某个任意线性方程的结果。

​		接下来，为了实现下一个卷积层，我们将实现一个1x1的卷积，最后我们还将用一个1x1的过滤器，然后是softmax激活，以输出一个1x1x4的体。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Convolutional_implementation_of_sliding_Windows1.PNG)

​		让我们来看一下你怎样用卷积的形式来实现滑动窗口目标识别。

​		假设你的卷积神经网络输入的是14x14的图像，或者说14x14x3的图像，而你测试集里的图像是16x16x3，也就是现在把这条黄色的边加到图像边缘，在原来的滑动窗口算法中，你可以把蓝色的区域输入到卷积神经网络中运行一次来得到分类结果0或1，然后向下滑动一点，你就需要运行这个卷积神经网络4次来得到4个标记，不过事实上，这里面的很多运行卷积神经网络的计算是重复的，所以对滑动窗口的卷积方式的实现，是让这4次卷积神经网络的前向运算共享计算过程。

​		具体来说，你可以这么做，你可以运行卷积神经网络，用同样的参数，最后你可以发现这个蓝色的1x1x4的子区域，包含的就是对左上的这个14x14的部分图像做卷积运算的结果，这个右上角的1x1x4的体包含右上部分的结果。那么这个过程所做的就是这个卷积形式的实现做的，就是不去强行在输入图像的4个子图像上分别进行前向传播，而是把4个合并成一个前向传播运算，从而利用这4个14x14的图像块的共同区域，共享了大量运算。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Convolutional_implementation_of_sliding_Windows2.PNG)

​		不过，这个算法仍然有一个弱点，那就是窗边框口的定位并不精确。

## 5. Bounding box predictions

​		如何精准的地预测边界框。在滑动窗口检测中，你通过这些预先决定的窗口来扫过整个图像，在这种情况下，没有一个边框与车的位置恰好吻合，另一方面，看上去一个更为精确的边框实际上不是一个正方形。	

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Bounding_box_predictions1.PNG)

​		YOLO算法就是一个比较好的能精确输出边界框的算法。

​		假设你有一张100x100的图片作为输入，接着你要将其用网格划分，出于演示的目的将使用3x3的网格，虽然在实际的操作中，你会使用更细分的网格。

​		基本思路就是你将把以前学习的图像分类及定位算法应用到这九个网格中的每一个，所以具体来说，下面是你要如何定义用于训练的标签，对于这九个网格中的每一个，你要给定一个标签y。YOLO算法是将这两个目标物分别分配到包含它们的中心点的网格元中，所以左边的汽车被分到了左边的网格元中，所以即使中间的网格元包含这两辆车的一部分，我们会假设中间的网格元不包含感兴趣的目标物，所以中间的网格元会有这样不包含目标物的标签y，所以对于这9个网格元中的每一个，你都会有一个8维的输出向量，由于你有3x3个网格元，输出的总大小将会是3x3x8。

​		这个算法的优势在于，该神经网络可以精确地输出如下边框。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Bounding_box_predictions2.PNG)

​		需要注意的是bx、by、bw、bh是相对网格指定的。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Bounding_box_predictions3.PNG)

## 6. Intersection over union

​		如何判断你的目标检测算法是否有效？交并比既可以用来评价你的目标检测算法，也可以用于往目标检测算法中加入其他特征部分来进一步改善它。

​		交并比做的就是它计算了这两个边界框的交集除以并集的比率。交并比是指先计算交集的大小，然后除以并集的面积，按计算机视觉的原则，如果交并比大于0.5，你的结果就会被判断为正确的，如果预测的和真实的边框完美重合了，交并比（IoU）就是1。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Intersection_over_union.PNG)

## 7. Non-max suppression

​		目前所学到的目标检测的问题之一是，你的算法或许会对同一目标有多次检测，所以与其只检测到目标一次或者会被检测到很多次，非极大值抑制是一种让你确保你的算法只对每个对象得到一次检测的方法。

​		图中19x19的网格，现在从技术上来说，这个车只有一个中间点，那么它应该被分配到一个网格单元中，所以从技术上来说，只有一个单元格预测出有一辆车，实际上，对每一个网格，你都进行目标分类和定位方法，也许一辆车会有多个网格确定车的中心点。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Non-max_suppression1.PNG)

​		因为你对每个网格使用图像分类和定位的算法，有可能许多网格会认为“我的pc值很大，即有个目标在我的网格中”，而不是有两个网格，在这391个网格中认为它们检测到了目标，所以当你运行算法时，对每个目标，你也许会得到多个检测结果，所以非极大值抑制要做的是清理这些检测。

​		这样每辆车只会得到一个检测结果，而不是多个结果，因此具体来说，它要做的是：首先，是看一看每个检测结果的相关概率，做完这一步，非极大值抑制再看所有的剩下的方框以及所有和你刚输入的那个结果有着多重叠，有着高交并比的方形区域，你得到的产出值将抑制。接下来，你将要计算所有剩余的方框来找出有着最大概率的那个，即最高pc值，接着非极大值抑制将去掉任何有着高交并比的。

​		那么现在，每个方框不是被高亮就是被调暗，如果你只是去掉那些调暗的方框，剩下的就是那些标记的，这是最终预测。

​		非最大值的意思是你将要输出有着最大可能性的分类判断，而抑制那些非最大可能性的邻近的方框。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Non-max_suppression2.PNG)

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Non-max_suppression3.PNG)

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Non-max_suppression4.PNG)

​		在本例中，将问题简化为只做汽车检测，所以去掉c1、c2、c3，而且现在假设每一个19x19网格的输出你将会获得如图的输出预测，那么为了运用非极大值抑制，首先要做的是丢掉所有方框，丢掉所有预测值pc小于或等于某个门限的边界值，这样就作用于所有的低概率输出框。接下来，没有被去掉或处理的，你将重复地选出有着最大概率的最大pc值的边框，将它作为一个预测结果。接下来，你要丢掉其他剩余边界框，即那些不被认为是预测结果的并且之前也没有去掉的边界框。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Non-max_suppression5.PNG)

## 8. Anchor boxes

​		到目前为止，对象检测中存在的一个问题就是每个格子只能检测出一个对象，如果你想让一个格子检测出多个对象，你可以这么做，就是使用anchor box这个概念。

​		在图中，汽车和行人的中点几乎在同一个地方，两者都落入到同一个格子中，所以对于那个格子，如果y输出这个向量，它将无法输出检测结果，所以必须从两个检测结果中选一个。

​		anchor box的思路是这样的，预先定义两个不同形状的anchor box，你要做的就是把预测结果和这两个anchor box关联起来，一般来说，你可能会用更多的anchor box，你需要重新定义一个y如图右边所示。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Anchor_boxes1.PNG)

​		总结：用anchor box之前，你做的是这个，对于训练集图像中的每个对象，都根据那个对象中点位置，分配到对应的格子里。现在用到anchor box概念是这么做的，现在每个对象都和之前一样分配到同一格子中，分配到对象中点所在格子中，但是它还分配到一个和对象形状交并比最高的anchor box，所以这里有两个anchor box，你就看看哪个anchor box和实际边框交并比高，不管选的是哪一个，这个对象不只分配到一个格子里，而是分配到一个（格子，anchor box）对，这就是对象在目标标签中的编码方式。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Anchor_boxes2.PNG)

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Anchor_boxes3.PNG)

​		我们建立anchor box这个概念是为了处理两个对象出现在同一格子的情况，实践中这种情况很少发生，特别是如果你用的是19x19而不是3x3的网格。也许设立anchor box的好处就是它能让你的学习算法能够更有针对性，特别是如果你的数据集有很高的对象和很宽的对象。

​		人们一般手动指定anchor box形状，你可以选择5个到10个anchor box的形状，覆盖到多种不同形状。

## 9. putting it together：YOLO algorithm

​		首先来看如何构造你的训练集，假设你要训练一个算法去检测三种对象，你还需要显式指定完整的背景类别，如果你要用两个anchor box，那么输出如图中y，要构造训练集，你需要遍历9个格子，然后构成对应的目标向量y。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\YOLO_algorithm1.PNG)

​		接下来看看算法是如何预测的。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\YOLO_algorithm2.PNG)

​		最后你要跑一下这个非极大值抑制，如果你使用两个anchor box，那么对于9个格子中任意一个都会有两个预测的边界框，其中一个的概率pc很低，注意有一些边界框可以超出所在格子的高度和宽度，接下来你将抛弃概率低的预测，如果你有多个对象需要检测，那么你要做的是，对于每个类别单独运行非极大值抑制，处理预测结果是那个类别的边界框。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\YOLO_algorithm3.PNG)

## 10. Region proposals（Optional）

​		如果你阅读对象检测的文献，可能会看到一组概念，所谓的候选区域，这在计算机视觉领域是非常有影响力的概念。

​		R-CNN算法是带区域的卷积网络，这个算法尝试选择出一些区域，在这些区域上运行卷积网络上是有意义的，所以这里不再针对每个滑动窗口跑检测算法，而是只选择一些窗口，在少数窗口上运行卷积神经网络。选出候选区域的方法是运行图像分割算法，分割的结果如图右边所示，为了找出可能存在对象的区域，需要先找出各个色块，然后在所有色块上放置边界框，然后在这些色块上运行分类器，这样需要处理的位置可能要少得多，可以减少运行分类器的时间。现在看来R-CNN还是很慢的。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Region_proposals1.PNG)

​		现在看来R-CNN还是很慢的，所以有一系列的研究工作去改进这个算法。基本的R-CNN算法是使用某些算法求出候选区域，然后对每个候选区域跑一下分类器，每个区域会输出一个标签，并输出一个边界框，这样你就能在确实存在的区域得到一个精确的边界框。现在R-CNN算法的一个缺点是太慢了，所以这些年有一些对R-CNN算法的改进工作。Fast R-CNN用卷积实现了滑动窗口，所以Fast R-CNN用的是滑动窗口的一个卷积实现，这显著提升了R-CNN的速度，事实证明，fast R-CNN的一个问题是得到候选区域的聚类步骤仍然非常缓慢，所以另一个研究组提出了Faster R-CNN使用的是卷积神经网络而不是传统的图像分割算法来获得候选区域色块，结果更快。

​		但是更快的R-CNN还是比YOLO算法慢很多。

![](C:\Users\Think\Desktop\吴恩达笔记\04-Convolution Neural Networks\week3\images\Region_proposals2.PNG)

