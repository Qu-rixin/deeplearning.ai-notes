# Logistic_Regression_with_a_Neural_Network_mindset_v3

## 1. 首先导入相关的包

在\python37\Scripts\目录中使用**pip install 包名**导入，如果下载失败，使用清华大学的镜像源下载。

```python
import numpy as np
import h5py

import matplotlib.pyplot as plt
import scipy
from PIL import Image
from scipy import ndimage
#from lr_utils import load_dataset
import skimage
```

## 2. 载入数据

```python
def load_dataset():
    train_dataset = h5py.File('datasets/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    # #训练集有209张64px*64px的图片
    # print("====train_set====")
    #print(train_set_x_orig)
    # print("scale: "+str(np.shape(train_set_x_orig)))

    # print("====train_set_label====")
    # #print(train_set_y_orig)
    # print("scale: "+str(np.shape(train_set_y_orig)))


    test_dataset = h5py.File('datasets/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    # #测试集有50张64px*64px的图片
    # print("====test_set====")
    # #print(test_set_x_orig)
    # print("scale: "+str(np.shape(test_set_x_orig)))

    # print("====test_set_label====")
    # #print(test_set_y_orig)
    # print("scale: "+str(np.shape(test_set_y_orig)))

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    # print("====list_classes====")
    # print(classes)
    # #前缀b表示byte类型的数据
    # print("scale: "+str(np.shape(classes)))
    
    #将列向量变成行向量
    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
```

## 3. 显示图片

```python
# Loading the data (cat/non-cat)
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()


# Example of a picture
index = 25
# Each line of your train_set_x_orig and test_set_x_orig is an array representing an image.
example = train_set_x_orig[index]
plt.imshow(train_set_x_orig[index])
plt.show()
```

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/deeplearning.ai-notes/week2/images/Cat_25.png)

## 4. Reshape

重新调整训练和测试数据集的形状，以便将大小（num-px，num-px，3）的图像展平为单个形状向量（num-px * num-px * 3，1）：

```python
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T
```

## 5. 预处理数据

标准化数据集，机器学习中一个常见的预处理步骤是将数据集居中并标准化，这意味着从每个示例中减去整个numpy数组的平均值，然后将每个示例除以整个numpy数组的标准差。但是对于图片数据集来说，它更简单、更方便，并且几乎可以将数据集的每一行除以255（像素通道的最大值）。

```python
train_set_x = train_set_x_flatten/255.
test_set_x = test_set_x_flatten/255.
```

# 学习算法的总体架构

逻辑回归实际上是一个非常简单的神经网络。

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/deeplearning.ai-notes/week2/images/General%20Architecture%20of%20the%20learning%20algorithm.png)

![](https://github.com/Qu-rixin/deeplearning.ai-notes/blob/master/deeplearning.ai-notes/week2/images/Mathematical%20expression%20of%20the%20algorithm.PNG)

算法的关键步骤：

1. 初始化模型的参数

2. 通过最小化成本学习模型的参数

3. 使用学习到的参数进行预测（在测试集上）

4. 分析结果并得出结论

## 1. 构建算法的主要部分

建立神经网络的主要步骤是：

1. 定义模型结构（例如输入特征的数量）

2. 初始化模型参数w、b

3. 循环：

   + 计算当前cost（正向传播）

   + 计算当前梯度（反向传播）

   + 更新参数（梯度下降）

## 2. 实现sigmoid函数

```python
def sigmoid(z):
    """
    Compute the sigmoid of z

    Arguments:
    z -- A scalar or numpy array of any size.

    Return:
    s -- sigmoid(z)
    """

    ### START CODE HERE ### (≈ 1 line of code)
    s = 1. / ( 1 + np.exp(-z))
    ### END CODE HERE ###
    
    return s
```

## 3. 初始化参数

```python
def initialize_with_zeros(dim):
    """
    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.
    
    Argument:
    dim -- size of the w vector we want (or number of parameters in this case)
    
    Returns:
    w -- initialized vector of shape (dim, 1)
    b -- initialized scalar (corresponds to the bias)
    """
    
    ### START CODE HERE ### (≈ 1 line of code)
    w = np.zeros(shape=(dim, 1), dtype=np.float32)
    b = 0
    ### END CODE HERE ###

    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
    
    return w, b
```

## 4. 前向与后向传播

现在参数被初始化，可以做“向前”和“向后”传播步骤来学习参数。

```python
def propagate(w, b, X, Y):
    """
    Implement the cost function and its gradient for the propagation

    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)

    Return:
    cost -- negative log-likelihood cost for logistic regression
    dw -- gradient of the loss with respect to w, thus same shape as w
    db -- gradient of the loss with respect to b, thus same shape as b
    
    Tips:
    - Write your code step by step for the propagation. np.log(), np.dot()
    """
    
    m = X.shape[1]
    
    # FORWARD PROPAGATION (FROM X TO COST)
    A = sigmoid(np.dot(w.T, X) + b)              # compute activation
    print(A)
    cost = (-1. / m) * np.sum((Y*np.log(A) + (1 - Y)*np.log(1-A)), axis=1)     # compute cost
    
    # BACKWARD PROPAGATION (TO FIND GRAD)
    dw = (1./m)*np.dot(X,((A-Y).T))
    db = (1./m)*np.sum(A-Y, axis=1)

    assert(dw.shape == w.shape)
    assert(db.dtype == float)
    cost = np.squeeze(cost)
    assert(cost.shape == ())
    
    #放到字典中
    grads = {"dw": dw,
             "db": db}
    
    return grads, cost
```

## 5. 优化

使用梯度下降法更新参数。

```python
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
    This function optimizes w and b by running a gradient descent algorithm
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of shape (num_px * num_px * 3, number of examples)
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
    num_iterations -- number of iterations of the optimization loop
    learning_rate -- learning rate of the gradient descent update rule(更新w、b时dw、db前的那个参数))
    print_cost -- True to print the loss every 100 steps
    
    Returns:
    params -- dictionary containing the weights w and bias b
    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
    
    """
    
    costs = []
    
    for i in range(num_iterations):
        
        
        # Cost and gradient calculation (≈ 1-4 lines of code)
        grads, cost = propagate(w=w, b=b, X=X, Y=Y)
        
        # Retrieve derivatives from grads
        dw = grads["dw"]
        db = grads["db"]
        
        # update parameters
        w = w - learning_rate*dw
        b = b -  learning_rate*db
        
        # Record the costs
        if i % 100 == 0:
            costs.append(cost)
        
        # Print the cost every 100 training examples
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
    
    params = {"w": w,
              "b": b}
    
    grads = {"dw": dw,
             "db": db}
    
    return params, grads, costs
```

## 6. 预测

```python
def predict(w, b, X):
    '''
    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    
    Returns:
    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
    '''
    
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)
    
    # Compute vector "A" predicting the probabilities of a cat being present in the picture
    A = sigmoid(np.dot(w.T, X) + b)
    
    [print(x) for x in A]
    for i in range(A.shape[1]):
        
        # Convert probabilities A[0,i] to actual predictions p[0,i]
        ### START CODE HERE ### (≈ 4 lines of code)
        if A[0, i] >= 0.5:
            Y_prediction[0, i] = 1
            
        else:
            Y_prediction[0, i] = 0
        ### END CODE HERE ###
    assert(Y_prediction.shape == (1, m))
    
    return Y_prediction
```

## 7.合并所有函数到一个模型里

```python
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
    """
    Builds the logistic regression model by calling the function you've implemented previously
    
    Arguments:
    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
    print_cost -- Set to true to print the cost every 100 iterations
    
    Returns:
    d -- dictionary containing information about the model.
    """
    
    # initialize parameters with zeros (≈ 1 line of code)
    w, b = initialize_with_zeros(X_train.shape[0])
    # Gradient descent (≈ 1 line of code)
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)
    
    # Retrieve parameters w and b from dictionary "parameters"
    w = parameters["w"]
    b = parameters["b"]
    
    # Predict test/train set examples (≈ 2 lines of code)
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)


    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

    
    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test, 
         "Y_prediction_train" : Y_prediction_train, 
         "w" : w, 
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}
    
    return d
```

## 8. 学习率的选择

为了使梯度下降起作用，你必须明智地选择学习率。学习率alpha决定我们更新参数的速度。如果学习率太大，我们可能会超过最优解。同样，如果它太小，我们将需要太多的迭代来收敛到最优解。这就是为什么使用调整好的学习率是至关重要的。

```python
learning_rates = [0.01, 0.001, 0.0001]
models = {}
for i in learning_rates:
    print ("learning rate is: " + str(i))
    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)
    print ('\n' + "-------------------------------------------------------" + '\n')

for i in learning_rates:
    plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))

plt.ylabel('cost')
plt.xlabel('iterations')

legend = plt.legend(loc='upper center', shadow=True)
frame = legend.get_frame()
frame.set_facecolor('0.90')
plt.show()
```

+ 不同的学习率会产生不同的cost，从而产生不同的预测结果。

+ 如果学习率太大（0.01），则cost可能上下波动。它甚至可能会发散（尽管在本例中，使用0.01最终还是会得到很好的成本值）。

+ 低cost并不意味着更好的模型。你得检查一下是否有过拟合的可能。当训练精度远高于测试精度时就会发生这种情况。

+ 在深入学习中，我们通常建议您：

  + 选择学习率，更好地最小化cost函数。

  + 如果您的模型过拟合，请使用其他技术来减少过拟合。

